{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-06-29 14:27:17,737:jax._src.xla_bridge:909: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import blackjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from blackjax.util import run_inference_algorithm\n",
    "from typing import NamedTuple\n",
    "\n",
    "logdensity_fn = lambda x: -jnp.sum(x**2)/2\n",
    "integrator = blackjax.mcmc.integrators.isokinetic_velocity_verlet\n",
    "target_acc_rate = 0.8\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "initial_position = jnp.ones(2)\n",
    "state = blackjax.nuts.init(initial_position, logdensity_fn)\n",
    "num_steps = 10000\n",
    "num_tuning_steps = 10000\n",
    "return_only_final = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobnikStepSizeTuningState(NamedTuple):\n",
    "    time : jnp.ndarray\n",
    "    step_size: float\n",
    "    x_average: float\n",
    "    step_size_max: float\n",
    "    num_dimensions: int\n",
    "\n",
    "def robnik_step_size_tuning(desired_energy_var, trust_in_estimate=1.5, num_effective_samples=150, step_size_max=jnp.inf):\n",
    "      \n",
    "    decay_rate = (num_effective_samples - 1.0) / (num_effective_samples + 1.0)\n",
    "\n",
    "    def init(initial_step_size, num_dimensions):\n",
    "        return RobnikStepSizeTuningState(time=0.0, x_average=0.0, step_size=initial_step_size, step_size_max=step_size_max, num_dimensions=num_dimensions)\n",
    "      \n",
    "    def update(robnik_state, energy_change):\n",
    "\n",
    "        xi = (\n",
    "            jnp.square(energy_change) / (robnik_state.num_dimensions * desired_energy_var)\n",
    "        ) + 1e-8  # 1e-8 is added to avoid divergences in log xi\n",
    "        weight = jnp.exp(\n",
    "            -0.5 * jnp.square(jnp.log(xi) / (6.0 * trust_in_estimate))\n",
    "        )  # the weight reduces the impact of stepsizes which are much larger on much smaller than the desired one.\n",
    "\n",
    "        x_average = decay_rate * robnik_state.x_average + weight * (\n",
    "            xi / jnp.power(robnik_state.step_size, 6.0)\n",
    "        )\n",
    "        \n",
    "        time = decay_rate * robnik_state.time + weight\n",
    "        step_size = jnp.power(\n",
    "            x_average / time, -1.0 / 6.0\n",
    "        )  # We use the Var[E] = O(eps^6) relation here.\n",
    "        step_size = (step_size < robnik_state.step_size_max) * step_size + (\n",
    "            step_size > robnik_state.step_size_max\n",
    "        ) * robnik_state.step_size_max  # if the proposed stepsize is above the stepsize where we have seen divergences\n",
    "\n",
    "        return RobnikStepSizeTuningState(time=time, x_average=x_average, step_size=step_size, step_size_max=step_size_max, num_dimensions=robnik_state.num_dimensions)\n",
    "\n",
    "\n",
    "    def final(robnik_state):\n",
    "        return robnik_state.step_size\n",
    "\n",
    "    return init, update, final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from typing import Callable, NamedTuple\n",
    "\n",
    "import blackjax.mcmc as mcmc\n",
    "from blackjax.adaptation.base import AdaptationResults, return_all_adapt_info\n",
    "from blackjax.adaptation.mass_matrix import (\n",
    "    MassMatrixAdaptationState,\n",
    "    mass_matrix_adaptation,\n",
    ")\n",
    "from blackjax.optimizers.dual_averaging import dual_averaging\n",
    "\n",
    "from blackjax.base import AdaptationAlgorithm\n",
    "from blackjax.progress_bar import gen_scan_fn\n",
    "from blackjax.types import Array, ArrayLikeTree, PRNGKey\n",
    "from blackjax.util import pytree_size\n",
    "from blackjax.adaptation.window_adaptation import build_schedule\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from blackjax.diagnostics import effective_sample_size\n",
    "# class DualAveragingAdaptationState(NamedTuple):\n",
    "#     log_step_size: float\n",
    "#     log_step_size_avg: float\n",
    "#     step: int\n",
    "#     avg_error: float\n",
    "#     mu: float\n",
    "\n",
    "# def dual_averaging_adaptation(\n",
    "#     target: float, t0: int = 10, gamma: float = 0.05, kappa: float = 0.75\n",
    "# ) -> tuple[Callable, Callable, Callable]:\n",
    "    \n",
    "#     da_init, da_update, da_final = dual_averaging(t0, gamma, kappa)\n",
    "\n",
    "#     def init(inital_step_size: float) -> DualAveragingAdaptationState:\n",
    "        \n",
    "#         return DualAveragingAdaptationState(*da_init(inital_step_size))\n",
    "\n",
    "#     def update(\n",
    "#         da_state: DualAveragingAdaptationState, value: float\n",
    "#     ) -> DualAveragingAdaptationState:\n",
    "        \n",
    "#         gradient = target - value\n",
    "#         return DualAveragingAdaptationState(*da_update(da_state, gradient))\n",
    "\n",
    "#     def final(da_state: DualAveragingAdaptationState) -> float:\n",
    "#         return jnp.exp(da_state.log_step_size_avg)\n",
    "\n",
    "#     return init, update, final\n",
    "\n",
    "class AlbaAdaptationState(NamedTuple):\n",
    "    ss_state: RobnikStepSizeTuningState  # step size\n",
    "    imm_state: MassMatrixAdaptationState  # inverse mass matrix\n",
    "    step_size: float\n",
    "    inverse_mass_matrix: Array\n",
    "    L : float\n",
    "\n",
    "def base(\n",
    "    is_mass_matrix_diagonal: bool,\n",
    "    v,\n",
    "    target_eevpd,\n",
    ") -> tuple[Callable, Callable, Callable]:\n",
    "    \n",
    "    mm_init, mm_update, mm_final = mass_matrix_adaptation(is_mass_matrix_diagonal)\n",
    "\n",
    "    # step_size_init, step_size_update, step_size_final = dual_averaging_adaptation(target_eevpd)\n",
    "    step_size_init, step_size_update, step_size_final = robnik_step_size_tuning(desired_energy_var=target_eevpd)\n",
    "\n",
    "    def init(\n",
    "        position: ArrayLikeTree,\n",
    "    ) -> AlbaAdaptationState:\n",
    "        \n",
    "        num_dimensions = pytree_size(position)\n",
    "        imm_state = mm_init(num_dimensions)\n",
    "\n",
    "        ss_state = step_size_init(initial_step_size=jnp.sqrt(num_dimensions)/5, num_dimensions=num_dimensions)\n",
    "\n",
    "        return AlbaAdaptationState(\n",
    "            ss_state,\n",
    "            imm_state,\n",
    "            ss_state.step_size,\n",
    "            imm_state.inverse_mass_matrix,\n",
    "            L = jnp.sqrt(num_dimensions)/v\n",
    "        )\n",
    "\n",
    "    def fast_update(\n",
    "        position: ArrayLikeTree,\n",
    "        value: float,\n",
    "        warmup_state: AlbaAdaptationState,\n",
    "    ) -> AlbaAdaptationState:\n",
    "        \"\"\"Update the adaptation state when in a \"fast\" window.\n",
    "\n",
    "        Only the step size is adapted in fast windows. \"Fast\" refers to the fact\n",
    "        that the optimization algorithms are relatively fast to converge\n",
    "        compared to the covariance estimation with Welford's algorithm\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        del position\n",
    "\n",
    "\n",
    "        new_ss_state =  step_size_update(warmup_state.ss_state, value)\n",
    "        new_step_size = new_ss_state.step_size # jnp.exp(new_ss_state.log_step_size)\n",
    "        \n",
    "        return AlbaAdaptationState(\n",
    "            new_ss_state,\n",
    "            warmup_state.imm_state,\n",
    "            new_step_size,\n",
    "            warmup_state.inverse_mass_matrix,\n",
    "            L = warmup_state.L\n",
    "        )\n",
    "\n",
    "    def slow_update(\n",
    "        position: ArrayLikeTree,\n",
    "        value: float,\n",
    "        warmup_state: AlbaAdaptationState,\n",
    "    ) -> AlbaAdaptationState:\n",
    "    \n",
    "        new_imm_state = mm_update(warmup_state.imm_state, position)\n",
    "        new_ss_state = step_size_update(warmup_state.ss_state, value)\n",
    "        new_step_size = new_ss_state.step_size # jnp.exp(new_ss_state.log_step_size)\n",
    "\n",
    "        return AlbaAdaptationState(\n",
    "            new_ss_state, new_imm_state, new_step_size, warmup_state.inverse_mass_matrix, L = warmup_state.L\n",
    "        )\n",
    "\n",
    "    def slow_final(warmup_state: AlbaAdaptationState) -> AlbaAdaptationState:\n",
    "\n",
    "        new_imm_state = mm_final(warmup_state.imm_state)\n",
    "        new_ss_state = step_size_init(step_size_final(warmup_state.ss_state), warmup_state.ss_state.num_dimensions)\n",
    "        new_step_size = new_ss_state.step_size # jnp.exp(new_ss_state.log_step_size)\n",
    "\n",
    "        new_L = jnp.sqrt(warmup_state.ss_state.num_dimensions)/v # \n",
    "\n",
    "        return AlbaAdaptationState(\n",
    "            new_ss_state,\n",
    "            new_imm_state,\n",
    "            new_step_size,\n",
    "            new_imm_state.inverse_mass_matrix,\n",
    "            L = new_L\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        adaptation_state: AlbaAdaptationState,\n",
    "        adaptation_stage: tuple,\n",
    "        position: ArrayLikeTree,\n",
    "        value: float,\n",
    "    ) -> AlbaAdaptationState:\n",
    "        \"\"\"Update the adaptation state and parameter values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adaptation_state\n",
    "            Current adptation state.\n",
    "        adaptation_stage\n",
    "            The current stage of the warmup: whether this is a slow window,\n",
    "            a fast window and if we are at the last step of a slow window.\n",
    "        position\n",
    "            Current value of the model parameters.\n",
    "        value\n",
    "            Value of the acceptance rate for the last mcmc step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The updated adaptation state.\n",
    "\n",
    "        \"\"\"\n",
    "        stage, is_middle_window_end = adaptation_stage\n",
    "\n",
    "        warmup_state = jax.lax.switch(\n",
    "            stage,\n",
    "            (fast_update, slow_update),\n",
    "            position,\n",
    "            value,\n",
    "            adaptation_state,\n",
    "        )\n",
    "\n",
    "        warmup_state = jax.lax.cond(\n",
    "            is_middle_window_end,\n",
    "            slow_final,\n",
    "            lambda x: x,\n",
    "            warmup_state,\n",
    "        )\n",
    "\n",
    "        return warmup_state\n",
    "\n",
    "    def final(warmup_state: AlbaAdaptationState) -> tuple[float, Array]:\n",
    "        \"\"\"Return the final values for the step size and mass matrix.\"\"\"\n",
    "        step_size = warmup_state.ss_state.step_size \n",
    "        # step_size = jnp.exp(warmup_state.ss_state.log_step_size_avg)\n",
    "        inverse_mass_matrix = warmup_state.imm_state.inverse_mass_matrix\n",
    "        L = warmup_state.L\n",
    "        return step_size, L, inverse_mass_matrix\n",
    "\n",
    "    return init, update, final\n",
    "\n",
    "def alba(\n",
    "    algorithm,\n",
    "    logdensity_fn: Callable,\n",
    "    target_eevpd,\n",
    "    v,\n",
    "    is_mass_matrix_diagonal: bool = True,\n",
    "    progress_bar: bool = False,\n",
    "    adaptation_info_fn: Callable = return_all_adapt_info,\n",
    "    integrator=mcmc.integrators.velocity_verlet,\n",
    "    num_alba_steps: int = 500,\n",
    "    alba_factor: float = 0.4,\n",
    "    **extra_parameters,\n",
    ") -> AdaptationAlgorithm:\n",
    "    \n",
    "\n",
    "    mcmc_kernel = algorithm.build_kernel(integrator)\n",
    "\n",
    "    adapt_init, adapt_step, adapt_final = base(\n",
    "        is_mass_matrix_diagonal,\n",
    "        target_eevpd=target_eevpd,\n",
    "        v=v,\n",
    "    )\n",
    "\n",
    "    def one_step(carry, xs):\n",
    "        _, rng_key, adaptation_stage = xs\n",
    "        state, adaptation_state = carry\n",
    "\n",
    "        new_state, info = mcmc_kernel(\n",
    "            rng_key=rng_key,\n",
    "            state=state,\n",
    "            logdensity_fn=logdensity_fn,\n",
    "            step_size=adaptation_state.step_size,\n",
    "            inverse_mass_matrix=adaptation_state.inverse_mass_matrix,\n",
    "            L=adaptation_state.L,\n",
    "            **extra_parameters,\n",
    "        )\n",
    "        new_adaptation_state = adapt_step(\n",
    "            adaptation_state,\n",
    "            adaptation_stage,\n",
    "            new_state.position,\n",
    "            info.energy_change,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (new_state, new_adaptation_state),\n",
    "            adaptation_info_fn(new_state, info, new_adaptation_state),\n",
    "        )\n",
    "\n",
    "    def run(rng_key: PRNGKey, position: ArrayLikeTree, num_steps: int = 1000):\n",
    "        init_key, rng_key, alba_key = jax.random.split(rng_key, 3)\n",
    "        init_state = algorithm.init(position, logdensity_fn, init_key)\n",
    "        init_adaptation_state = adapt_init(position)\n",
    "\n",
    "        if progress_bar:\n",
    "            print(\"Running window adaptation\")\n",
    "        scan_fn = gen_scan_fn(num_steps-num_alba_steps, progress_bar=progress_bar)\n",
    "        start_state = (init_state, init_adaptation_state)\n",
    "        keys = jax.random.split(rng_key, num_steps-num_alba_steps)\n",
    "        schedule = build_schedule(num_steps-num_alba_steps)\n",
    "        last_state, info = scan_fn(\n",
    "            one_step,\n",
    "            start_state,\n",
    "            (jnp.arange(num_steps-num_alba_steps), keys, schedule),\n",
    "        )\n",
    "\n",
    "        last_chain_state, last_warmup_state, *_ = last_state\n",
    "        step_size, L, inverse_mass_matrix = adapt_final(last_warmup_state)\n",
    "\n",
    "        ###\n",
    "        ### ALBA TUNING\n",
    "        ###\n",
    "        keys = jax.random.split(alba_key, num_alba_steps)\n",
    "        mcmc_kernel = algorithm.build_kernel(integrator)\n",
    "        def step(state, key):\n",
    "            next_state, _ = mcmc_kernel(\n",
    "                rng_key=key,\n",
    "                state=state,\n",
    "                logdensity_fn=logdensity_fn,\n",
    "                L=L,\n",
    "                step_size=step_size,\n",
    "                inverse_mass_matrix=inverse_mass_matrix,\n",
    "            )\n",
    "\n",
    "            return next_state, next_state.position\n",
    "        \n",
    "        if num_alba_steps > 0:\n",
    "            _, samples = jax.lax.scan(step, last_chain_state, keys)\n",
    "            flat_samples = jax.vmap(lambda x: ravel_pytree(x)[0])(samples)\n",
    "            ess = effective_sample_size(flat_samples[None, ...])\n",
    "\n",
    "            L=alba_factor * step_size * jnp.mean(num_alba_steps / ess)\n",
    "        \n",
    "\n",
    "        parameters = {\n",
    "            \"step_size\": step_size,\n",
    "            \"inverse_mass_matrix\": inverse_mass_matrix,\n",
    "            \"L\": L,\n",
    "            **extra_parameters,\n",
    "        }\n",
    "\n",
    "        return (\n",
    "            AdaptationResults(\n",
    "                last_chain_state,\n",
    "                parameters,\n",
    "            ),\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    return AdaptationAlgorithm(run)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blackjax.adaptation.step_size import (\n",
    "    dual_averaging_adaptation,\n",
    ")\n",
    "from blackjax.mcmc.adjusted_mclmc_dynamic import rescale\n",
    "\n",
    "\n",
    "def make_random_trajectory_length_fn(random_trajectory_length : bool):\n",
    "    if random_trajectory_length:\n",
    "        integration_steps_fn = lambda avg_num_integration_steps: lambda k: jnp.ceil(\n",
    "            jax.random.uniform(k) * rescale(avg_num_integration_steps)\n",
    "        ).astype('int32')\n",
    "    else:\n",
    "        integration_steps_fn = lambda avg_num_integration_steps: lambda _: jnp.ceil(\n",
    "            avg_num_integration_steps\n",
    "        ).astype('int32')\n",
    "    return integration_steps_fn\n",
    "\n",
    "def da_adaptation(\n",
    "    algorithm,\n",
    "    logdensity_fn: Callable,\n",
    "    integration_steps_fn: Callable,\n",
    "    inverse_mass_matrix,\n",
    "    initial_step_size: float = 1.0,\n",
    "    target_acceptance_rate: float = 0.80,\n",
    "    integrator=blackjax.mcmc.integrators.velocity_verlet, \n",
    "):\n",
    "    \n",
    "    da_init, da_update, da_final = dual_averaging_adaptation(target_acceptance_rate)\n",
    "    kernel = algorithm.build_kernel(integrator=integrator)\n",
    "    \n",
    "    def step(state, key):\n",
    "\n",
    "        adaptation_state, kernel_state = state\n",
    "\n",
    "        new_kernel_state, info = kernel(\n",
    "            rng_key=key,\n",
    "            state=kernel_state,\n",
    "            logdensity_fn=logdensity_fn,\n",
    "            step_size=jnp.exp(adaptation_state.log_step_size),\n",
    "            inverse_mass_matrix=inverse_mass_matrix,\n",
    "            integration_steps_fn=integration_steps_fn,\n",
    "        )\n",
    "\n",
    "        new_adaptation_state = da_update(\n",
    "            adaptation_state,\n",
    "            info.acceptance_rate,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (new_adaptation_state, new_kernel_state),\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def run(rng_key: PRNGKey, position: ArrayLikeTree, num_steps: int = 1000):\n",
    "\n",
    "\n",
    "        init_key, rng_key = jax.random.split(rng_key)\n",
    "        \n",
    "        init_kernel_state = algorithm.init(position, logdensity_fn, init_key)\n",
    "\n",
    "        keys = jax.random.split(rng_key, num_steps)\n",
    "        init_state = da_init(initial_step_size), init_kernel_state\n",
    "        (adaptation_state, kernel_state), info = jax.lax.scan(\n",
    "            step,\n",
    "            init_state,\n",
    "            keys,\n",
    "        )\n",
    "        step_size = da_final(adaptation_state)\n",
    "        return (\n",
    "            kernel_state,\n",
    "            {\n",
    "                \"step_size\": step_size,\n",
    "                \"inverse_mass_matrix\": inverse_mass_matrix,\n",
    "            },\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    return AdaptationAlgorithm(run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_alba(\n",
    "    unadjusted_algorithm,\n",
    "    logdensity_fn: Callable,\n",
    "    target_eevpd,\n",
    "    v,\n",
    "    adjusted_algorithm,\n",
    "    num_dimensions: int,\n",
    "    integrator,\n",
    "    target_acceptance_rate: float = 0.80,\n",
    "    num_alba_steps: int = 500,\n",
    "    alba_factor: float = 0.4,\n",
    "    **extra_parameters,\n",
    "    ):\n",
    "\n",
    "    unadjusted_warmup = alba(\n",
    "        algorithm= unadjusted_algorithm,\n",
    "        logdensity_fn=logdensity_fn,\n",
    "        target_eevpd=target_eevpd,\n",
    "        v=v,\n",
    "        integrator=integrator,\n",
    "        num_alba_steps=num_alba_steps,\n",
    "        alba_factor=alba_factor, **extra_parameters)\n",
    "    \n",
    "    def run(rng_key: PRNGKey, position: ArrayLikeTree, num_steps: int = 1000):\n",
    "        \n",
    "        unadjusted_warmup_key, adjusted_warmup_key = jax.random.split(rng_key)\n",
    "\n",
    "        (state, params), adaptation_info = unadjusted_warmup.run(unadjusted_warmup_key, position, num_steps)\n",
    "\n",
    "        avg_num_integration_steps = params[\"L\"] / params[\"step_size\"]\n",
    "\n",
    "        integration_steps_fn = lambda k: jnp.ceil(\n",
    "                    jax.random.uniform(k) * rescale(avg_num_integration_steps)\n",
    "                )\n",
    "\n",
    "        adjusted_warmup = da_adaptation(\n",
    "            algorithm=adjusted_algorithm,\n",
    "            logdensity_fn=logdensity_fn,\n",
    "            integration_steps_fn=integration_steps_fn,\n",
    "            initial_step_size=params[\"step_size\"],\n",
    "            target_acceptance_rate=target_acceptance_rate,\n",
    "            inverse_mass_matrix=params[\"inverse_mass_matrix\"],\n",
    "            integrator=integrator, **extra_parameters)\n",
    "        \n",
    "        state, params, adaptation_info = adjusted_warmup.run(adjusted_warmup_key, state.position, num_steps)\n",
    "        params[\"L\"] = adaptation_info.num_integration_steps.mean()*params[\"step_size\"]\n",
    "        return state, params, adaptation_info\n",
    "    \n",
    "    return AdaptationAlgorithm(run)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-06-30 12:37:59,118:jax._src.xla_bridge:909: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'step_size': Array(0.636016, dtype=float32),\n",
       " 'inverse_mass_matrix': Array([1.1305283 , 0.97562194], dtype=float32),\n",
       " 'L': Array(1.373382, dtype=float32)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from blackjax.adaptation.adjusted_abla import adjusted_alba\n",
    "from blackjax.adaptation.unadjusted_step_size import robnik_step_size_tuning\n",
    "from blackjax.adaptation.unadjusted_alba import unadjusted_alba\n",
    "\n",
    "import blackjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from blackjax.util import run_inference_algorithm\n",
    "from typing import NamedTuple\n",
    "\n",
    "logdensity_fn = lambda x: -jnp.sum(x**2)/2\n",
    "integrator = blackjax.mcmc.integrators.isokinetic_velocity_verlet\n",
    "target_acc_rate = 0.8\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "initial_position = jnp.ones(2)\n",
    "state = blackjax.nuts.init(initial_position, logdensity_fn)\n",
    "num_steps = 10000\n",
    "num_tuning_steps = 10000\n",
    "return_only_final = True\n",
    "\n",
    "\n",
    "\n",
    "alg = blackjax.mclmc\n",
    "warmup_key = jax.random.PRNGKey(0)\n",
    "warmup = unadjusted_alba(algorithm=alg, logdensity_fn=logdensity_fn, integrator=integrator, target_eevpd=5e-4, v=1., num_alba_steps=5000)\n",
    "(state, params), adaptation_info = warmup.run(warmup_key, initial_position, num_tuning_steps)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_size': Array(1.722377, dtype=float32),\n",
       " 'inverse_mass_matrix': Array([1.0370758, 1.0950992], dtype=float32),\n",
       " 'L': Array(3.3546734, dtype=float32)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup = adjusted_alba(\n",
    "    unadjusted_algorithm=blackjax.mclmc,\n",
    "    logdensity_fn=logdensity_fn,\n",
    "    target_eevpd=5e-4,\n",
    "    v=1.,\n",
    "    adjusted_algorithm=blackjax.adjusted_mclmc_dynamic,\n",
    "    target_acceptance_rate=0.8,\n",
    "    num_dimensions=2,\n",
    "    integrator=blackjax.mcmc.integrators.isokinetic_velocity_verlet\n",
    ")\n",
    "\n",
    "state, params, adaptation_info = warmup.run(warmup_key, initial_position, num_tuning_steps)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_size': Array(1.2649875, dtype=float32),\n",
       " 'inverse_mass_matrix': Array([1.0368719, 1.1122135], dtype=float32),\n",
       " 'L': Array(2.4872184, dtype=float32)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup = adjusted_alba(\n",
    "    unadjusted_algorithm=blackjax.langevin,\n",
    "    logdensity_fn=logdensity_fn,\n",
    "    target_eevpd=5e-4,\n",
    "    v=jnp.sqrt(2),\n",
    "    adjusted_algorithm=blackjax.dynamic_malt,\n",
    "    target_acceptance_rate=0.8,\n",
    "    num_dimensions=2,\n",
    "    integrator=blackjax.mcmc.integrators.velocity_verlet\n",
    ")\n",
    "\n",
    "state, params, adaptation_info = warmup.run(warmup_key, initial_position, num_tuning_steps)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adjusted_warmup_key = jax.random.PRNGKey(0)\n",
    "\n",
    "\n",
    "# integration_steps_fn = lambda k: jnp.ceil(\n",
    "#             jax.random.uniform(k) * rescale(avg_num_integration_steps)\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "# warmup = da_adaptation(\n",
    "#                 # rng_key=adjusted_warmup_key,\n",
    "#                 # initial_position=state.position,\n",
    "#                 algorithm=blackjax.adjusted_mclmc_dynamic,\n",
    "#                 integrator=integrator,\n",
    "#                 logdensity_fn=logdensity_fn,\n",
    "#                 num_dimensions=pytree_size(state.position),\n",
    "#                 # num_steps=num_tuning_steps,\n",
    "#                 target_acceptance_rate=target_acc_rate,\n",
    "#                 initial_step_size=params[\"step_size\"],\n",
    "#                 integration_steps_fn=integration_steps_fn,\n",
    "#             )\n",
    "\n",
    "# state, params, adaptation_info = warmup.run(adjusted_warmup_key, state.position, num_tuning_steps)\n",
    "\n",
    "# params[\"L\"] = adaptation_info.num_integration_steps.mean()*params[\"step_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobnikStepSizeTuningState(time=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, step_size=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, x_average=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, step_size_max=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, num_dimensions=Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
      "RobnikStepSizeTuningState(time=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, step_size=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, x_average=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, step_size_max=inf, num_dimensions=Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
      "RobnikStepSizeTuningState(time=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, step_size=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, x_average=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, step_size_max=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, num_dimensions=Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
      "RobnikStepSizeTuningState(time=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, step_size=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, x_average=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, step_size_max=inf, num_dimensions=Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
      "params: {'L': Array(1.3128282, dtype=float32), 'inverse_mass_matrix': Array([1.0370758, 1.0950992], dtype=float32), 'step_size': Array(0.6757085, dtype=float32)}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_size': Array(1.722377, dtype=float32),\n",
       " 'inverse_mass_matrix': Array([1.0370758, 1.0950992], dtype=float32),\n",
       " 'L': Array(3.3546734, dtype=float32)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_size': Array(1.798047, dtype=float32),\n",
       " 'inverse_mass_matrix': Array([1., 1.], dtype=float32),\n",
       " 'L': Array(3.502056, dtype=float32)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
